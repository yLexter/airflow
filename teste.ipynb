{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pendulum\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import uuid\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "siglas = ['PS', 'ATD', 'AM']\n",
    "anos = [2023, 2024]\n",
    "estados = ['PB']\n",
    "meses = [f\"{i:02}\" for i in range(1, 13)]\n",
    "path_parquets = 'sia-parquet/sia-pb-2023-2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDag:\n",
    "\n",
    "    pattern = r'(?P<sigla>[A-Z]{2,3})(?P<estado>[A-Z]{2})(?P<ano>\\d{2})(?P<mes>\\d{2}).parquet'\n",
    "\n",
    "    json_drop_columns = 'dags/jsons/SIA/drop_columns.json'\n",
    "    json_group_transformation = 'dags/jsons/SIA/group_transformation.json'\n",
    "    json_rename_columns = 'dags/jsons/SIA/rename_columns.json'\n",
    "\n",
    "    bucket_bronze = 'bronze'\n",
    "    base_folder_cache = 'dags/cache'\n",
    "    minio_client = boto3.client(\n",
    "           's3',\n",
    "            endpoint_url='http://10.100.100.61:9000',\n",
    "            aws_access_key_id='minioadmin',\n",
    "            aws_secret_access_key='minioadmin',\n",
    "            region_name='us-east-1',\n",
    "    )\n",
    "\n",
    "\n",
    "    def read_json(path):\n",
    "        json = None\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            json = json.load(f)\n",
    "\n",
    "        return json\n",
    "\n",
    "    def get_folder_cache(self):\n",
    "        return str(uuid.uuid4())[:14]\n",
    "\n",
    "    def list_folders_minio(self, path):\n",
    "        folders = set()\n",
    "        continuation_token = None\n",
    "\n",
    "        while True:\n",
    "            list_params = { \n",
    "                \"Bucket\": self.bucket_bronze, \n",
    "                \"Prefix\": path \n",
    "            }\n",
    "\n",
    "            if continuation_token:\n",
    "                list_params[\"ContinuationToken\"] = continuation_token\n",
    "\n",
    "            response = self.minio_client.list_objects_v2(**list_params)\n",
    "\n",
    "            if \"Contents\" in response:\n",
    "                for obj in response[\"Contents\"]:\n",
    "                    key = obj[\"Key\"]\n",
    "                    parts = key.split(\"/\")[:-1]  \n",
    "\n",
    "                    for i in range(1, len(parts) + 1):\n",
    "                        folders.add(\"/\".join(parts[:i]) + \"/\")\n",
    "\n",
    "            if response.get(\"IsTruncated\"):\n",
    "                continuation_token = response[\"NextContinuationToken\"]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return sorted(folders)\n",
    "\n",
    "    def create_filenames(self):\n",
    "        return [f\"{sigla}{estado}{ano % 100}{mes}\" for sigla, ano, estado, mes in itertools.product(siglas, anos, estados, meses)]\n",
    "\n",
    "    def filter_paths(self, paths, codigos):\n",
    "        paths_filtrados = []\n",
    "        \n",
    "        for path in paths:\n",
    "            path_lower = path.lower()  \n",
    "            \n",
    "            for codigo in codigos:\n",
    "                codigo_lower = codigo.lower()\n",
    "                if codigo_lower in path_lower:\n",
    "                    paths_filtrados.append(path)\n",
    "                    break \n",
    "\n",
    "        return paths_filtrados\n",
    "\n",
    "    def download_parquet(self):\n",
    "        all_parquets = self.list_folders_minio(path_parquets)\n",
    "        files = self.create_filenames()\n",
    "\n",
    "        filtred_parquets = self.filter_paths(all_parquets, files)\n",
    "        \n",
    "        for path_parquet_minio in filtred_parquets[:1]:\n",
    "            \n",
    "            folder_parquet = re.search(self.pattern, path_parquet_minio).group()\n",
    "            path_parquet_cache = os.path.join(self.folder_cache_files, folder_parquet)\n",
    "\n",
    "            if not os.path.exists(path_parquet_cache):\n",
    "                os.mkdir(path_parquet_cache)\n",
    "            else:\n",
    "                shutil.rmtree(path_parquet_cache)\n",
    "                os.mkdir(path_parquet_cache)\n",
    "            \n",
    "            response = self.minio_client.list_objects_v2(Bucket=self.bucket_bronze, Prefix= path_parquet_minio )\n",
    "\n",
    "            if 'Contents' in response:\n",
    "                for obj in response['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    file_name = os.path.basename(key)\n",
    "\n",
    "                    file_path = os.path.join(path_parquet_cache, file_name)\n",
    "                    \n",
    "                    self.minio_client.download_file(Bucket=self.bucket_bronze, Key=key, Filename=file_path)\n",
    "    \n",
    "    def drop_columns(self, parquet_path):\n",
    "        \n",
    "        drop_columns_json = self.read_json(self.json_drop_columns)\n",
    "\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        df = df.drop(columns=drop_columns_json, errors='ignore')\n",
    "\n",
    "        df.to_parquet(parquet_path)\n",
    "\n",
    "    def rename_columns(self, parquet_path):\n",
    "        json_columns = self.read_json(self.json_rename_columns)\n",
    "\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        df = df.rename(columns=json_columns)\n",
    "\n",
    "        df.to_parquet(parquet_path)\n",
    "    \n",
    "    def group_transformation(self, parquet_path):\n",
    "        json_columns_mapping = self.read_json(self.json_group_transformation)\n",
    "\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "\n",
    "        for column_name, mappings in json_columns_mapping.items():\n",
    "            if column_name in df.columns:\n",
    "                df[column_name] = df[column_name].map(mappings).fillna(df[column_name])\n",
    "\n",
    "        df.to_parquet(parquet_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dags/cache/AMPB2301.parquet'\n",
    "base = BaseDag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m base\u001b[38;5;241m.\u001b[39mrename_columns(path)\n\u001b[0;32m      3\u001b[0m base\u001b[38;5;241m.\u001b[39mgroup_transformation(path)\n",
      "Cell \u001b[1;32mIn[12], line 111\u001b[0m, in \u001b[0;36mBaseDag.drop_columns\u001b[1;34m(self, parquet_path)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_drop_columns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    109\u001b[0m     drop_columns \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m--> 111\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mdrop_columns, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    114\u001b[0m df\u001b[38;5;241m.\u001b[39mto_parquet(parquet_path)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:651\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[0;32m    500\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    510\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m    654\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    657\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:67\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     65\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "base.drop_columns(path)\n",
    "#base.rename_columns(path)\n",
    "#base.group_transformation(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
